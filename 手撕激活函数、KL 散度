import numpy as np


# Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Tanh
def tanh(x):
    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

# MAE
def MAE(y_real, y_pred):
    return np.mean(np.abs(y_real - y_pred))

# MSE
def MSE(y_real, y_pred):
    return np.mean((y_real - y_pred) ** 2)

# Binary Cross-Entropy
def binary_cross_entropy(y_real, y_pred, eps=1e-15):
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -np.mean(y_real * np.log(y_pred) + (1 - y_real) * np.log(1 - y_pred))

# Softmax
def softmax(logits):
    exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))
    sum_exp_logits = np.sum(exp_logits, axis=-1, keepdims=True)
    softmax_output = exp_logits / sum_exp_logits
    return softmax_output

# KL Divergence
def kl_div(p, q, esp=1e-15):
    return np.sum(p * np.log((p + esp) / (q + esp)))

y_t = np.array([[1/6,1/6,1/6,1/6,1/6,1/6], [1/6,1/6,1/6,1/6,1/6,1/6]])
y_p = np.array([[1/2, 1/10, 1/10, 1/10, 1/10, 1/10], [1/6,1/6,1/6,1/6,1/6,1/6]])
print(MSE(y_p, y_t))
